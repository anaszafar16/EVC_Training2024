{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task3: Named Entity Detection\n",
        "\n",
        "Preprocess the following text dataset for a Named Entity Recognition (NER) task by performing NLP techniques such as tokenization, sentence segmentation, and annotating each token with the appropriate BIO (Beginning, Inside, Outside) tags according to the entities present (PERSON, ORGANIZATION, LOCATION, MISC)\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/alaakhaled/conll003-englishversion/data?select=test.txt\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lgO-CBKluBRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "-4LWEZvA34yD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-crfsuite\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "Udk0x3PV6qzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n",
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download alaakhaled/conll003-englishversion\n",
        "!unzip /content/conll003-englishversion.zip\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "dnqXRf9T35QQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics\n",
        "from nltk.corpus.reader import ConllCorpusReader"
      ],
      "metadata": {
        "id": "-R4r4fX2DRCZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/'\n",
        "\n",
        "train = ConllCorpusReader(path, 'train.txt', ['words', 'pos', 'chunk', 'ne'])\n",
        "valid = ConllCorpusReader(path, 'valid.txt', ['words', 'pos', 'chunk', 'ne'])\n",
        "test = ConllCorpusReader(path, 'test.txt', ['words', 'pos', 'chunk', 'ne'])"
      ],
      "metadata": {
        "id": "ie0jJwwH5SY_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_conll_data(file_path):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            if line.strip():\n",
        "                word, pos, chunk, ne = line.strip().split()\n",
        "                sentence.append((word, pos, chunk, ne))\n",
        "            else:\n",
        "                sentences.append(sentence)\n",
        "                sentence = []\n",
        "        if sentence:\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "train_data = read_conll_data('train.txt')\n",
        "valid_data = read_conll_data('valid.txt')\n",
        "test_data = read_conll_data('test.txt')\n"
      ],
      "metadata": {
        "id": "KG31-EGL6VGR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "L-x2ZbBO6Zj3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, chunk, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, chunk, label in sent]"
      ],
      "metadata": {
        "id": "ShTo-mVtDZ52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [sent2features(s) for s in train_data]\n",
        "y_train = [sent2labels(s) for s in train_data]\n",
        "X_valid = [sent2features(s) for s in valid_data]\n",
        "y_valid = [sent2labels(s) for s in valid_data]\n",
        "X_test = [sent2features(s) for s in test_data]\n",
        "y_test = [sent2labels(s) for s in test_data]"
      ],
      "metadata": {
        "id": "epuZXS7wDcLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "crf_model = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "\n",
        "crf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = crf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "qM-Siqt56pAj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(crf_model.classes_)\n",
        "labels.remove('O')\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMCANWBE7EG7",
        "outputId": "977be1a1-1f5b-469b-84b2-3e0d35a86404"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'B-ORG', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.flat_f1_score(y_test, y_pred,\n",
        "                      average='weighted', labels=labels)\n",
        "\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83_qVry268RY",
        "outputId": "09678081-7bc6-462b-9d22-c7e1bc87858c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC      0.851     0.814     0.832      1668\n",
            "       I-LOC      0.744     0.646     0.692       257\n",
            "      B-MISC      0.817     0.764     0.789       702\n",
            "      I-MISC      0.702     0.667     0.684       216\n",
            "       B-ORG      0.772     0.728     0.749      1661\n",
            "       I-ORG      0.684     0.734     0.708       835\n",
            "       B-PER      0.828     0.851     0.839      1617\n",
            "       I-PER      0.866     0.949     0.905      1156\n",
            "\n",
            "   micro avg      0.805     0.801     0.803      8112\n",
            "   macro avg      0.783     0.769     0.775      8112\n",
            "weighted avg      0.805     0.801     0.802      8112\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentence for prediction\n",
        "input_sentence = 'EU rejects German call to boycott British lamb.'\n",
        "tokens = input_sentence.split()\n",
        "\n",
        "# Preprocess the sample sentence\n",
        "sample_features = sent2features(tokens)\n",
        "\n",
        "# Predict using the trained CRF model\n",
        "sample_prediction = crf_model.predict([sample_features])[0]\n",
        "\n",
        "# Display the results\n",
        "for word, label in zip(tokens, sample_prediction):\n",
        "    print(f\"{word}: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDdL-_OCCzMB",
        "outputId": "2a76944d-4a76-4c37-9d1c-cede40d35058"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU: O\n",
            "rejects: O\n",
            "German: O\n",
            "call: O\n",
            "to: O\n",
            "boycott: O\n",
            "British: O\n",
            "lamb.: O\n"
          ]
        }
      ]
    }
  ]
}